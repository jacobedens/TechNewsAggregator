I want to build a tech news aggregator where I can take multiple sources for example youtube sources, blog posts from openAI and antropic. I want to scrape these put them into a database where we have some structure where we have sources where we have lets call them articles. Then I want to run a daily digest where we are going to take all of the articles from within the timeframe, do an LLM summary around that and then based on the user insights that we specify in some kind of agent system prompt we can generate a daily digest which is going to be short snippets with a link to the original source. From the youtube channels I want to be able to create a list of channels and I want to get the latest videos from those channels. I think we can use the youtube RSS feed for that. For the blog posts we can use URLs. Ok so I want everything built in a Python backend. I want to use a postgresSQL database. I want to use SQL alchemy in order to define the database models and create tables. I want the project structure to be in an app folder where all of the app logic is in and I also want a docker folder where we first create the very minimal setup for the postgresSQL database. That will probably be the starting point. WE want to make sure that later on down the line we can deploy the whole app to render and then also schedule it every 24hrs to run reports get everything and then when we've created the daily digest I want to send an email to my personal inbox with this.